{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from esda.moran import Moran, Moran_Local\n",
    "from esda.moran import Moran_Local\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy import stats\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from spreg import OLS, ML_Lag, ML_Error\n",
    "import geopandas as gpd\n",
    "import libpysal\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17971a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(r\"C:\\Users\\16643\\Desktop\\data\\trip_merged_dataset.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Data Cleaning & Mapping: Income Groups and Main Mode Categories\n",
    "# =============================================================================\n",
    "\n",
    "income_mapping = {\n",
    "    1: \"Less than £25,000\",\n",
    "    2: \"£25,000 to £49,999\", \n",
    "    3: \"£50,000 and over\"\n",
    "}\n",
    "\n",
    "\n",
    "mode_group_map = {\n",
    "    1: \"Active Travel\", 2: \"Active Travel\",           # Walking, Cycling\n",
    "    3: \"Private Vehicle\", 4: \"Private Vehicle\",       # Car driver/passenger\n",
    "    5: \"Private Vehicle\", 6: \"Private Vehicle\",       # Motorcycle, Other private\n",
    "    7: \"Bus\", 8: \"Bus\", 9: \"Bus\",                     # Local bus, Non-local bus, Coach\n",
    "    10: \"Rail / Underground\", 11: \"Rail / Underground\", # Surface rail, Underground\n",
    "    12: \"Taxi / Other Public\", 13: \"Taxi / Other Public\" # Taxi, Other public\n",
    "}\n",
    "\n",
    "\n",
    "merged_df[\"HHIncomeGroup\"] = merged_df[\"HHIncome2002_B02ID\"].map(income_mapping)\n",
    "merged_df['MainMode_Group'] = merged_df['MainMode_B04ID'].map(mode_group_map)\n",
    "\n",
    "merged_df_clean = merged_df[\n",
    "    (merged_df['HHIncomeGroup'].notna()) &\n",
    "    (merged_df['MainMode_Group'].notna()) &\n",
    "    (merged_df['TripTotalTime'] >= 1) & \n",
    "    (merged_df['TripTotalTime'] <= 180)\n",
    "].copy()\n",
    "\n",
    "\n",
    "income_order = ['Less than £25,000', '£25,000 to £49,999', '£50,000 and over']\n",
    "merged_df_clean['HHIncomeGroup'] = pd.Categorical(merged_df_clean['HHIncomeGroup'], \n",
    "                                                  categories=income_order, ordered=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure: Mode Share by Income & Trip Duration Distribution\n",
    "# =============================================================================\n",
    "\n",
    "mpl.rcParams['font.family'] = 'serif' \n",
    "mpl.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'Times', 'Georgia', 'STSong', 'SimSun']\n",
    "mpl.rcParams['font.size'] = 10\n",
    "mpl.rcParams['axes.unicode_minus'] = False  \n",
    "\n",
    "mpl.rcParams['text.color'] = 'black'\n",
    "mpl.rcParams['axes.labelcolor'] = 'black'\n",
    "mpl.rcParams['axes.titlecolor'] = 'black'\n",
    "mpl.rcParams['xtick.color'] = 'black'\n",
    "mpl.rcParams['ytick.color'] = 'black'\n",
    "\n",
    "sns.set_theme(style='whitegrid', rc={\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'DejaVu Serif', 'Times', 'Georgia', 'STSong', 'SimSun'],\n",
    "    'font.size': 10\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9, 12))  \n",
    "\n",
    "ax1 = axes[0]\n",
    "mode_prop = merged_df_clean.groupby(['HHIncomeGroup', 'MainMode_Group']).size().unstack(fill_value=0)\n",
    "mode_prop_pct = mode_prop.div(mode_prop.sum(axis=1), axis=0) * 100\n",
    "\n",
    "mode_order = ['Active Travel', 'Bus', 'Taxi / Other Public', 'Rail / Underground', 'Private Vehicle']\n",
    "mode_prop_pct = mode_prop_pct.reindex(columns=mode_order, fill_value=0)\n",
    "\n",
    "colors = ['#34495e', '#5d6d7e', '#85929e', '#aeb6bf', '#d5dbdb']\n",
    "mode_prop_pct.plot(kind='bar', stacked=True, ax=ax1, color=colors, width=0.65, \n",
    "                   edgecolor='white', linewidth=0.5)\n",
    "\n",
    "ax1.set_title('Transport Mode Choice by Income Level', \n",
    "              fontweight='normal', fontsize=15, pad=20)\n",
    "ax1.set_xlabel('Household Income Group', fontweight='normal')\n",
    "ax1.set_ylabel('Percentage of Trips (%)', fontweight='normal')\n",
    "ax1.legend(title='Transport Mode\\n(Speed: Slow → Fast)', \n",
    "           bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9,\n",
    "           title_fontsize=9, frameon=True, fancybox=True, shadow=False)\n",
    "\n",
    "_wrap_map = {\n",
    "    'Less than £25,000': 'Less than\\n£25,000',\n",
    "    '£25,000 to £49,999': '£25,000 to\\n£49,999',\n",
    "    '£50,000 and over': '£50,000 and\\nover'\n",
    "}\n",
    "_xtlabs = [t.get_text() for t in ax1.get_xticklabels()]\n",
    "ax1.set_xticklabels([_wrap_map.get(t, t) for t in _xtlabs], rotation=0, ha='center')\n",
    "\n",
    "for i, income in enumerate(income_order):\n",
    "    active_bus_pct = mode_prop_pct.loc[income, ['Active Travel', 'Bus']].sum()\n",
    "    private_pct = mode_prop_pct.loc[income, 'Private Vehicle']\n",
    "    \n",
    "    ax1.text(i, 10, f'{active_bus_pct:.0f}%\\nSlow modes', ha='center', va='bottom', \n",
    "             fontweight='normal', color='white', fontsize=9,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='#2c3e50', alpha=0.9))\n",
    "    \n",
    "    ax1.text(i, 65, f'{private_pct:.0f}% car', ha='center', va='top',\n",
    "             fontweight='normal', color='white', fontsize=9,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='#27ae60', alpha=0.9))\n",
    "\n",
    "ax2 = axes[1]\n",
    "\n",
    "contrast_data = merged_df_clean[\n",
    "    merged_df_clean['MainMode_Group'].isin(['Active Travel', 'Bus', 'Private Vehicle'])\n",
    "].copy()\n",
    "\n",
    "contrast_data['Mode_Category'] = contrast_data['MainMode_Group'].map({\n",
    "    'Private Vehicle': 'Private Car\\n(High income)',\n",
    "    'Active Travel': 'Walking/Cycling\\n(Low income)',\n",
    "    'Bus': 'Bus\\n(Low income)'\n",
    "})\n",
    "\n",
    "speed_order = ['Private Car\\n(High income)', 'Walking/Cycling\\n(Low income)', 'Bus\\n(Low income)']\n",
    "\n",
    "palette = {\n",
    "    'Private Car\\n(High income)': '#2c3e50',\n",
    "    'Walking/Cycling\\n(Low income)': '#7f8c8d',\n",
    "    'Bus\\n(Low income)': '#95a5a6'\n",
    "}\n",
    "\n",
    "sns.boxplot(data=contrast_data, x='Mode_Category', y='TripTotalTime', ax=ax2,\n",
    "            order=speed_order, palette=palette, width=0.6)\n",
    "\n",
    "ax2.set_title('Travel Time Distribution by Transport Mode', \n",
    "              fontweight='normal', fontsize=15, pad=20)\n",
    "ax2.set_xlabel('Transport Mode', fontweight='normal')\n",
    "ax2.set_ylabel('Trip Duration (minutes)', fontweight='normal')\n",
    "ax2.tick_params(axis='x', labelsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "mode_stats = []\n",
    "for i, category in enumerate(speed_order):\n",
    "    subset = contrast_data[contrast_data['Mode_Category'] == category]\n",
    "    if len(subset) > 0:\n",
    "        mean_time = subset['TripTotalTime'].mean()\n",
    "        mode_stats.append((category, mean_time))\n",
    "        \n",
    "        ax2.text(i, ax2.get_ylim()[1] * 0.92, \n",
    "                 f'Mean: {mean_time:.1f} min', \n",
    "                 ha='center', va='top', fontweight='normal', fontsize=10,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='#ecf0f1', \n",
    "                           edgecolor='#bdc3c7', alpha=0.95))\n",
    "\n",
    "plt.tight_layout(h_pad=7)\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "for ax in axes:\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_color('#95a5a6')\n",
    "    ax.spines['bottom'].set_color('#95a5a6')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6daedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Dataset Assembly: Load PTAL/IMD, Map LSOA11→LSOA21, Clean PTAL Grades, Merge Geo\n",
    "# =============================================================================\n",
    "\n",
    "ptal_df = pd.read_csv(r\"C:\\Users\\16643\\Desktop\\data\\LSOA_aggregated_PTAL_stats_2023.csv\")\n",
    "ptal_df['LSOA21CD'] = ptal_df['LSOA21CD'].astype(str).str.strip()\n",
    "\n",
    "imd_df = pd.read_csv(r\"C:\\Users\\16643\\Desktop\\data\\london_imd.csv\")\n",
    "imd_df = imd_df.rename(columns={\n",
    "    'LSOA code (2011)': 'LSOA11CD',\n",
    "    'Index of Multiple Deprivation (IMD) quintile rebased for London': 'IMD_quintile_london'\n",
    "})\n",
    "\n",
    "crosswalk_df = pd.read_csv(\n",
    "    r\"C:\\Users\\16643\\Desktop\\data\\LSOA_(2011)_to_LSOA_(2021)_to_Local_Authority_District_(2022)_Exact_Fit_Lookup_for_EW_(V3).csv\"\n",
    ")\n",
    "crosswalk_df = crosswalk_df[['LSOA11CD', 'LSOA21CD']]\n",
    "\n",
    "imd_2021 = imd_df.merge(crosswalk_df, on='LSOA11CD', how='left')\n",
    "\n",
    "\n",
    "imd_cols = [\n",
    "    'LSOA21CD',\n",
    "    'IMD_quintile_london',\n",
    "    'Income_london_decile',\n",
    "    'employment_london_decile',\n",
    "    'edu_london_decile',\n",
    "    'health_london_decile',\n",
    "    'crime_london_decile',\n",
    "    'barriers_london_decile',\n",
    "    'livingEnv_london_decile'\n",
    "]\n",
    "\n",
    "imd_2021 = imd_2021[imd_cols]\n",
    "\n",
    "imd_2021 = imd_2021.groupby('LSOA21CD', as_index=False).mean()\n",
    "\n",
    "lsoa21_gdf = gpd.read_file(r\"C:\\Users\\16643\\Desktop\\data\\LSOA2021\\LSOA_2021_EW_BSC_V4.shp\")\n",
    "lsoa21_gdf['LSOA21CD'] = lsoa21_gdf['LSOA21CD'].astype(str).str.strip()\n",
    "imd_2021['LSOA21CD'] = imd_2021['LSOA21CD'].astype(str).str.strip()\n",
    "\n",
    "lsoa21_gdf = lsoa21_gdf[lsoa21_gdf['LSOA21CD'].isin(ptal_df['LSOA21CD'])].copy()\n",
    "\n",
    "merged_gdf = lsoa21_gdf.merge(\n",
    "    ptal_df[['LSOA21CD', 'MEAN_PTAL_']],\n",
    "    on='LSOA21CD',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "merged_gdf = merged_gdf.merge(imd_2021, on='LSOA21CD', how='left')\n",
    "\n",
    "\n",
    "def clean_ptal(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    \n",
    "    val_str = str(val).strip().lower()\n",
    "    \n",
    "\n",
    "    if 'b' in val_str:\n",
    "        try:\n",
    "            base_num = float(val_str.replace('b', ''))\n",
    "            return base_num + 0.5\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    " \n",
    "    if 'a' in val_str:\n",
    "        try:\n",
    "            base_num = float(val_str.replace('a', ''))\n",
    "            return 0.5 if base_num == 1 else base_num\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "  \n",
    "    try:\n",
    "        return float(val_str)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "merged_gdf['MEAN_PTAL_CLEAN'] = merged_gdf['MEAN_PTAL_'].apply(clean_ptal)\n",
    "\n",
    "merged_gdf_clean = merged_gdf.dropna(subset=['MEAN_PTAL_CLEAN']).copy()\n",
    "\n",
    "print(f\"The final number of LSOAs for analysis: {len(merged_gdf_clean)}\")\n",
    "\n",
    "merged_gdf_clean['IMD_quintile_london'] = merged_gdf_clean['IMD_quintile_london'].round().astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure: Public Transport Accessibility (PTAL) and Deprivation (IMD) Maps\n",
    "# =============================================================================\n",
    "\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'Times', 'Georgia', 'STSong', 'SimSun']\n",
    "mpl.rcParams['font.size'] = 12\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid', rc={'font.family': 'serif', 'font.size': 12})\n",
    "\n",
    "# 1: PTAL map\n",
    "fig1, ax1 = plt.subplots(1, 1, figsize=(10, 10))\n",
    "merged_gdf_clean.plot(\n",
    "    column='MEAN_PTAL_CLEAN',\n",
    "    cmap='YlGnBu',\n",
    "    legend=True,\n",
    "    legend_kwds={\n",
    "        'label': \"Public Transport Accessibility Level (PTAL)\",\n",
    "        'orientation': \"vertical\",\n",
    "        'shrink': 0.5\n",
    "    },\n",
    "    edgecolor='grey',\n",
    "    linewidth=0.3,\n",
    "    ax=ax1,\n",
    "    missing_kwds={'color': 'lightgrey'}\n",
    ")\n",
    "ax1.set_title(\n",
    "    \"Public Transport Accessibility Level (PTAL) in London by LSOA\",\n",
    "    fontsize=15, pad=20\n",
    ")\n",
    "ax1.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2: IMD map\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(10, 10))\n",
    "merged_gdf_clean.plot(\n",
    "    column='IMD_quintile_london',\n",
    "    cmap='OrRd',\n",
    "    linewidth=0.2,\n",
    "    edgecolor='grey',\n",
    "    legend=True,\n",
    "    legend_kwds={\n",
    "        'label': \"Index of Multiple Deprivation (IMD) Quintile\",\n",
    "        'orientation': \"vertical\",\n",
    "        'shrink': 0.5\n",
    "    },\n",
    "    ax=ax2,\n",
    "    missing_kwds={'color': 'lightgrey'}\n",
    ")\n",
    "ax2.set_title(\n",
    "    \"Index of Multiple Deprivation (IMD) Quintiles in London by LSOA\",\n",
    "    fontsize=15, pad=20\n",
    ")\n",
    "ax2.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure: London Transport–Deprivation Spatial Patterns and Correlation Analysis\n",
    "# =============================================================================\n",
    "\n",
    "if 'IMD_quintile_london' not in merged_gdf_clean.columns:\n",
    "    raise KeyError(\"IMD_quintile_london column missing!\")\n",
    "if 'MEAN_PTAL_CLEAN' not in merged_gdf_clean.columns:\n",
    "    raise KeyError(\"MEAN_PTAL_CLEAN column missing!\")\n",
    "\n",
    "med_imd = merged_gdf_clean['IMD_quintile_london'].median()\n",
    "med_ptal = merged_gdf_clean['MEAN_PTAL_CLEAN'].median()\n",
    "\n",
    "merged_gdf_clean['high_deprivation'] = merged_gdf_clean['IMD_quintile_london'] > med_imd\n",
    "merged_gdf_clean['high_connectivity'] = merged_gdf_clean['MEAN_PTAL_CLEAN'] > med_ptal\n",
    "\n",
    "def assign_quadrant(row):\n",
    "    if row['high_deprivation'] and row['high_connectivity']:\n",
    "        return 'Transport Paradox'\n",
    "    elif row['high_deprivation'] and not row['high_connectivity']:\n",
    "        return 'Expected Deprivation'\n",
    "    elif not row['high_deprivation'] and row['high_connectivity']:\n",
    "        return 'Privileged Access'\n",
    "    else:\n",
    "        return 'Suburban Pattern'\n",
    "\n",
    "merged_gdf_clean['quadrant'] = merged_gdf_clean.apply(assign_quadrant, axis=1).astype(\"category\")\n",
    "\n",
    "\n",
    "quadrant_stats = merged_gdf_clean['quadrant'].value_counts()\n",
    "quadrant_pct = merged_gdf_clean['quadrant'].value_counts(normalize=True) * 100\n",
    "\n",
    "\n",
    "for quad, count in quadrant_stats.items():\n",
    "    pct = quadrant_pct[quad]\n",
    "    print(f\"{quad}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "\n",
    "correlation = merged_gdf_clean['IMD_quintile_london'].corr(merged_gdf_clean['MEAN_PTAL_CLEAN'])\n",
    "\n",
    "print(f\"\\nCorrelation analysis:\")\n",
    "print(f\"Correlation coefficient between IMD quintile and PTAL: {correlation:.4f}\")\n",
    "\n",
    "if abs(correlation) < 0.1:\n",
    "    print(\"-> Very weak correlation, confirming Centre for London's findings!\")\n",
    "elif abs(correlation) < 0.3:\n",
    "    print(\"-> Weak correlation\")\n",
    "else:\n",
    "    print(\"-> Moderate or strong correlation\")\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "colors = {\n",
    "    'Transport Paradox': '#d62728',\n",
    "    'Expected Deprivation': '#ff7f0e',\n",
    "    'Privileged Access': '#2ca02c',\n",
    "    'Suburban Pattern': '#1f77b4'\n",
    "}\n",
    "\n",
    "for quadrant, color in colors.items():\n",
    "    subset = merged_gdf_clean[merged_gdf_clean['quadrant'] == quadrant]\n",
    "    if len(subset) > 0:\n",
    "        subset.plot(ax=ax, color=color, edgecolor='white', linewidth=0.1)\n",
    "\n",
    "legend_handles = [\n",
    "    mpatches.Patch(color=color, label=f\"{quad} (n={len(merged_gdf_clean[merged_gdf_clean['quadrant']==quad]):,})\")\n",
    "    for quad, color in colors.items()\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_handles, loc='upper left', bbox_to_anchor=(0, 1))\n",
    "ax.set_title(\"London Transport-Deprivation Spatial Pattern\", fontsize=16, pad=20)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n=== Descriptive statistics of each quadrant ===\")\n",
    "quadrant_means = merged_gdf_clean.groupby('quadrant')[['MEAN_PTAL_CLEAN', 'IMD_quintile_london']]\\\n",
    "    .agg(['mean','std','min','max']).round(2)\n",
    "print(quadrant_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d915fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Statistical Test: Kruskal–Wallis Analysis of PTAL and IMD across Quadrants\n",
    "# =============================================================================\n",
    "\n",
    "groups_ptal = [g['MEAN_PTAL_CLEAN'].dropna()\n",
    "               for _, g in merged_gdf_clean.groupby('quadrant')]\n",
    "H, p_kw = kruskal(*groups_ptal)\n",
    "print('Kruskal-Wallis PTAL: H =', H, 'p =', p_kw)\n",
    "\n",
    "groups_imd = [g['IMD_quintile_london'].dropna()\n",
    "              for _, g in merged_gdf_clean.groupby('quadrant')]\n",
    "H_imd, p_imd = kruskal(*groups_imd)\n",
    "print('Kruskal-Wallis IMD: H =', H_imd, 'p =', p_imd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f70a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Identification of Extreme Cases \n",
    "# =============================================================================\n",
    "\n",
    "# Transport Paradox: Top 10 LSOAs with the highest connectivity\n",
    "paradox_top = merged_gdf_clean[merged_gdf_clean['quadrant']=='Transport Paradox'].nlargest(10, 'MEAN_PTAL_CLEAN')\n",
    "print(\"Top 10 LSOAs with the highest connectivity in the Transport Paradox quadrant:\")\n",
    "for i, (idx, row) in enumerate(paradox_top.iterrows(),1):\n",
    "    print(f\"{i:2d}. {row['LSOA21NM'][:30]:<30} PTAL:{row['MEAN_PTAL_CLEAN']:.1f} IMD:{row['IMD_quintile_london']}\")\n",
    "\n",
    "# Expected Deprivation: Bottom 10 LSOAs with the lowest connectivity\n",
    "expected_worst = merged_gdf_clean[merged_gdf_clean['quadrant']=='Expected Deprivation'].nsmallest(10, 'MEAN_PTAL_CLEAN')\n",
    "print(\"\\nBottom 10 LSOAs with the lowest connectivity in the Expected Deprivation quadrant:\")\n",
    "for i, (idx, row) in enumerate(expected_worst.iterrows(),1):\n",
    "    print(f\"{i:2d}. {row['LSOA21NM'][:30]:<30} PTAL:{row['MEAN_PTAL_CLEAN']:.1f} IMD:{row['IMD_quintile_london']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d478a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Correlation Analysis: PTAL vs IMD Domains + Quadrant Deprivation Profiles (Radar Chart)\n",
    "# =============================================================================\n",
    "\n",
    "imd_domains = {\n",
    "    \"Income\": \"Income_london_decile\",\n",
    "    \"Employment\": \"employment_london_decile\",\n",
    "    \"Education\": \"edu_london_decile\",\n",
    "    \"Health\": \"health_london_decile\",\n",
    "    \"Crime\": \"crime_london_decile\",\n",
    "    \"Housing\": \"barriers_london_decile\",\n",
    "    \"Environment\": \"livingEnv_london_decile\"\n",
    "}\n",
    "\n",
    "\n",
    "def interpret_correlation(r, p):\n",
    "    if abs(r) < 0.1:\n",
    "        strength = \"Very weak\"\n",
    "    elif abs(r) < 0.3:\n",
    "        strength = \"Weak\"\n",
    "    elif abs(r) < 0.5:\n",
    "        strength = \"Moderate\"\n",
    "    else:\n",
    "        strength = \"Strong\"\n",
    "    direction = \"positive\" if r > 0 else \"negative\"\n",
    "    if p < 0.05:\n",
    "        return f\"{strength} {direction}\"\n",
    "    else:\n",
    "        return \"Non-significant\"\n",
    "\n",
    "results = []\n",
    "for domain, col in imd_domains.items():\n",
    "    corr, pval = pearsonr(merged_gdf_clean['MEAN_PTAL_CLEAN'], merged_gdf_clean[col])\n",
    "    results.append({\n",
    "        \"Domain\": domain,\n",
    "        \"Correlation\": round(corr,3),\n",
    "        \"P_value\": \"<0.001\" if pval < 0.001 else f\"{pval:.3f}\",\n",
    "        \"Interpretation\": interpret_correlation(corr, pval)\n",
    "    })\n",
    "\n",
    "correlation_results = pd.DataFrame(results)\n",
    "print(\"\\n=== Correlation results with Interpretation ===\")\n",
    "print(correlation_results.to_string(index=False))\n",
    "\n",
    "\n",
    "if \"quadrant\" in merged_gdf_clean.columns:\n",
    "    paradox_profile = merged_gdf_clean[merged_gdf_clean[\"quadrant\"]==\"Transport Paradox\"][list(imd_domains.values())].mean()\n",
    "    other_quadrants = merged_gdf_clean.groupby(\"quadrant\")[list(imd_domains.values())].mean()\n",
    "\n",
    "    print(\"\\n=== Transport Paradox Profile ===\")\n",
    "    print(paradox_profile)\n",
    "\n",
    "    print(\"\\n=== Other Quadrants Profile ===\")\n",
    "    print(other_quadrants)\n",
    "\n",
    "\n",
    "\n",
    "if \"quadrant\" in merged_gdf_clean.columns:\n",
    "    labels = list(imd_domains.keys())\n",
    "    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
    "    angles += angles[:1] \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,7), subplot_kw=dict(polar=True))\n",
    "\n",
    "    colors = {\n",
    "        \"Transport Paradox\": \"r\",\n",
    "        \"Expected Deprivation\": \"b\",\n",
    "        \"Privileged Access\": \"g\",\n",
    "        \"Suburban Pattern\": \"orange\"\n",
    "    }\n",
    "\n",
    "    for quad, row in other_quadrants.iterrows():\n",
    "        vals = [row[col] for col in imd_domains.values()]\n",
    "        vals += vals[:1]\n",
    "        ax.plot(angles, vals, color=colors.get(quad, \"gray\"), linewidth=2, label=quad)\n",
    "        ax.fill(angles, vals, color=colors.get(quad, \"gray\"), alpha=0.1)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_title(\"Deprivation Profiles by Quadrant\")\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3,1.1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function: Calculate Improved Policy Priority Score \n",
    "# (Normalized PTAL & IMD, Quadrant Weights, Composite Index, Priority Levels)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_improved_priority_score(df):\n",
    "    \"\"\"\n",
    "    Improved policy priority calculation with enhanced differentiation and practicality\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Normalize PTAL\n",
    "    df['ptal_normalized'] = (df['MEAN_PTAL_CLEAN'] - df['MEAN_PTAL_CLEAN'].min()) / \\\n",
    "                           (df['MEAN_PTAL_CLEAN'].max() - df['MEAN_PTAL_CLEAN'].min())\n",
    "    \n",
    "    # Normalize IMD quintile (1–5 mapped to 0–1)\n",
    "    df['imd_normalized'] = (df['IMD_quintile_london'] - 1) / 4  \n",
    "\n",
    "    # Paradox intensity = high deprivation but low connectivity\n",
    "    df['paradox_intensity'] = df['imd_normalized'] * (1 - df['ptal_normalized'])\n",
    "    \n",
    "    # Base quadrant weights\n",
    "    quadrant_weights = {\n",
    "        'Transport Paradox': 1.0,      # Highest priority\n",
    "        'Expected Deprivation': 0.85,  # High priority\n",
    "        'Suburban Pattern': 0.4,       # Medium priority\n",
    "        'Privileged Access': 0.1       # Low priority\n",
    "    }\n",
    "    df['base_priority'] = df['quadrant'].map(quadrant_weights).astype(float)\n",
    "    \n",
    "    # Weighted score combining multiple factors, normalized to 0–1\n",
    "    df['policy_priority_score'] = (\n",
    "        df['base_priority'] * 3.0 +       # Quadrant weight\n",
    "        df['imd_normalized'] * 2.0 +      # Level of deprivation\n",
    "        (1 - df['ptal_normalized']) * 1.5 +  # Lack of transport accessibility\n",
    "        df['paradox_intensity'] * 1.0     # Weighted paradox intensity\n",
    "    ) / 7.5  \n",
    "    \n",
    "    # Rescale to 1–10\n",
    "    df['policy_priority_score'] = df['policy_priority_score'] * 9 + 1\n",
    "    \n",
    "    # Assign categorical priority levels\n",
    "    df['priority_level'] = pd.cut(\n",
    "        df['policy_priority_score'], \n",
    "        bins=[0, 3, 5, 7, 8.5, 10], \n",
    "        labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112193c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_gdf_improved = calculate_improved_priority_score(merged_gdf_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b28831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure: PTAL vs IMD Scatter Plot with Quadrant Classification and Trend Line\n",
    "# =============================================================================\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Times New Roman',\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 14,\n",
    "    'axes.linewidth': 1.2,\n",
    "    'grid.alpha': 0.3,\n",
    "    'axes.edgecolor': 'black'\n",
    "})\n",
    "\n",
    "\n",
    "# Figure 1: PTAL vs IMD Scatter Plot with Quadrant Analysis\n",
    "\n",
    "def create_figure1_ptal_imd_scatter(data):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    \n",
    "   \n",
    "    colors_quad_academic = {\n",
    "        'Transport Paradox': '#d62728',      \n",
    "        'Expected Deprivation': '#ff7f0e',   \n",
    "        'Privileged Access': '#2ca02c',      \n",
    "        'Suburban Pattern': '#1f77b4'        \n",
    "    }\n",
    "    \n",
    "    med_imd = data['IMD_quintile_london'].median()\n",
    "    med_ptal = data['MEAN_PTAL_CLEAN'].median()\n",
    "    \n",
    "\n",
    "    for quad, color in colors_quad_academic.items():\n",
    "        subset = data[data['quadrant'] == quad]\n",
    "        if len(subset) > 0:\n",
    "            if quad == 'Transport Paradox' and 'paradox_intensity' in subset.columns:\n",
    "                scatter = ax.scatter(\n",
    "                    subset['MEAN_PTAL_CLEAN'], \n",
    "                    subset['IMD_quintile_london'],\n",
    "                    c=subset['paradox_intensity'],\n",
    "                    cmap='Reds',\n",
    "                    label=f'{quad} (n={len(subset):,})',\n",
    "                    alpha=0.7, \n",
    "                    s=25, \n",
    "                    edgecolor='white', \n",
    "                    linewidth=0.3\n",
    "                )\n",
    "            else:\n",
    "                scatter = ax.scatter(\n",
    "                    subset['MEAN_PTAL_CLEAN'], \n",
    "                    subset['IMD_quintile_london'],\n",
    "                    color=color,\n",
    "                    label=f'{quad} (n={len(subset):,})',\n",
    "                    alpha=0.7, \n",
    "                    s=25, \n",
    "                    edgecolor='white', \n",
    "                    linewidth=0.3\n",
    "                )\n",
    "    \n",
    "\n",
    "    ax.axhline(y=med_imd, color='black', linestyle='--', alpha=0.7, linewidth=1.5,\n",
    "               label=f'IMD Median = {med_imd:.1f}')\n",
    "    ax.axvline(x=med_ptal, color='black', linestyle='--', alpha=0.7, linewidth=1.5,\n",
    "               label=f'PTAL Median = {med_ptal:.1f}')\n",
    "\n",
    "    ax.text(0.02, 0.98, 'Expected\\nDeprivation', transform=ax.transAxes, \n",
    "            fontsize=10, ha='left', va='top', \n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    ax.text(0.98, 0.98, 'Transport\\nParadox', transform=ax.transAxes, \n",
    "            fontsize=10, ha='right', va='top',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    ax.text(0.02, 0.02, 'Privileged\\nAccess', transform=ax.transAxes, \n",
    "            fontsize=10, ha='left', va='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    ax.text(0.98, 0.02, 'Suburban\\nPattern', transform=ax.transAxes, \n",
    "            fontsize=10, ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "\n",
    "    X = data['MEAN_PTAL_CLEAN'].values.reshape(-1, 1)\n",
    "    y = data['IMD_quintile_london'].values\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    trend_x = np.linspace(data['MEAN_PTAL_CLEAN'].min(), data['MEAN_PTAL_CLEAN'].max(), 100)\n",
    "    trend_y = reg.predict(trend_x.reshape(-1, 1))\n",
    "    ax.plot(trend_x, trend_y, 'k-', alpha=0.5, linewidth=2, \n",
    "            label=f'Trend (R² = {reg.score(X, y):.3f})')\n",
    "\n",
    "    ax.set_xlabel('Public Transport Accessibility Level (PTAL)')\n",
    "    ax.set_ylabel('Index of Multiple Deprivation Quintile (London)')\n",
    "    ax.set_title('Transport-Deprivation Relationship in Greater London\\n(Quadrant Classification)', \n",
    "                 pad=20)\n",
    "    \n",
    "    ax.set_xlim(0, data['MEAN_PTAL_CLEAN'].max() * 1.05)\n",
    "    ax.set_ylim(0.8, 5.2)\n",
    "    \n",
    "\n",
    "    ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "    \n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=True, fancybox=True)\n",
    "    \n",
    "\n",
    "    ax.text(0.02, 0.88, f'Total LSOAs: {len(data):,}', transform=ax.transAxes,\n",
    "            fontsize=10, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================= \n",
    "# Figure: Policy Intervention Priority Index Map \n",
    "# =============================================================================\n",
    "\n",
    "# Set serif font globally\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'Times', 'Georgia', 'STSong', 'SimSun']\n",
    "mpl.rcParams['font.size'] = 10\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def create_figure2_priority_map(gdf_data):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    \n",
    "    colors_priority_academic = {\n",
    "        'Very High': '#8B0000',\n",
    "        'High': '#DC143C',\n",
    "        'Medium': '#FF8C00',\n",
    "        'Low': '#228B22',\n",
    "        'Very Low': '#4682B4'\n",
    "    }\n",
    "    \n",
    "    priority_order = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "    handles = []\n",
    "    \n",
    "    for level in priority_order:\n",
    "        subset = gdf_data[gdf_data['priority_level'] == level]\n",
    "        if len(subset) > 0:\n",
    "            p = subset.plot(\n",
    "                ax=ax, \n",
    "                color=colors_priority_academic[level], \n",
    "                label=f'{level} (n={len(subset):,})',\n",
    "                edgecolor='white', \n",
    "                linewidth=0.1, \n",
    "                alpha=0.85\n",
    "            )\n",
    "            # manually collect handle for legend\n",
    "            handles.append(plt.Line2D([0], [0], color=colors_priority_academic[level], \n",
    "                                      lw=6, label=f'{level} (n={len(subset):,})'))\n",
    "    \n",
    "    ax.set_title(\n",
    "        'Spatial Distribution of Transport-Deprivation Policy Priority Index\\nin Greater London', \n",
    "        fontsize=15, pad=25, weight='bold'\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    legend = ax.legend(\n",
    "        handles=handles,\n",
    "        title='Policy Priority Level',\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(0.02, 0.98),\n",
    "        frameon=True,\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        fontsize=11,\n",
    "        title_fontsize=12\n",
    "    )\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_alpha(0.95)\n",
    "    \n",
    "    # simple scale bar\n",
    "    scale_bar = Rectangle(\n",
    "        (0.02, 0.02), 0.15, 0.03, transform=ax.transAxes,\n",
    "        facecolor='white', edgecolor='black', alpha=0.9\n",
    "    )\n",
    "    ax.add_patch(scale_bar)\n",
    "    ax.text(\n",
    "        0.095, 0.035, '10 km', transform=ax.transAxes, \n",
    "        ha='center', va='center', fontsize=9, weight='bold'\n",
    "    )\n",
    "    \n",
    "    # north arrow\n",
    "    ax.annotate(\n",
    "        'N', xy=(0.95, 0.95), xytext=(0.95, 0.9),\n",
    "        arrowprops=dict(arrowstyle='->', lw=2, color='black'),\n",
    "        transform=ax.transAxes, ha='center', va='center',\n",
    "        fontsize=14, weight='bold'\n",
    "    )\n",
    "    \n",
    "    # data source\n",
    "    ax.text(\n",
    "        0.02, 0.06, \n",
    "        'Data: TfL PTAL 2021, MHCLG IMD 2019\\nMethod: Multi-factor weighted analysis',\n",
    "        transform=ax.transAxes, fontsize=9, \n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9)\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a62dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure : Statistical Analysis Heatmap\n",
    "# =============================================================================\n",
    "\n",
    "def create_figure3_statistical_heatmap(data):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    crosstab = pd.crosstab(data['quadrant'], data['priority_level'])\n",
    "    quadrant_order = ['Transport Paradox', 'Expected Deprivation', 'Suburban Pattern', 'Privileged Access']\n",
    "    priority_order = ['Very High', 'High', 'Medium', 'Low', 'Very Low']\n",
    "    crosstab = crosstab.reindex(index=quadrant_order, columns=priority_order, fill_value=0)\n",
    "    \n",
    "    sns.heatmap(crosstab, annot=True, fmt='d', cmap='YlOrRd', ax=ax1,\n",
    "                cbar_kws={'label': 'Number of LSOAs'}, \n",
    "                linewidths=0.5, linecolor='white')\n",
    "    ax1.set_title('Quadrant vs Priority Level Distribution', fontweight='bold', pad=15)\n",
    "    ax1.set_xlabel('Policy Priority Level')\n",
    "    ax1.set_ylabel('Transport-Deprivation Quadrant')\n",
    "    \n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(crosstab)\n",
    "    ax1.text(0.02, 0.98, f'χ² = {chi2:.1f}, p < 0.001***', \n",
    "             transform=ax1.transAxes, fontsize=10, weight='bold',\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    box_data = [data[data['quadrant'] == quad]['policy_priority_score'].dropna() \n",
    "                for quad in quadrant_order]\n",
    "    bp = ax2.boxplot(box_data, labels=quadrant_order, patch_artist=True)\n",
    "    \n",
    "    colors = ['#d62728', '#ff7f0e', '#1f77b4', '#2ca02c']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax2.set_title('Policy Priority Score Distribution by Quadrant', fontweight='bold', pad=15)\n",
    "    ax2.set_ylabel('Policy Priority Score (1-10)')\n",
    "    ax2.set_xlabel('Transport-Deprivation Quadrant')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    f_stat, p_val = stats.f_oneway(*box_data)\n",
    "    ax2.text(0.02, 0.98, f'ANOVA: F = {f_stat:.1f}, p < 0.001***', \n",
    "             transform=ax2.transAxes, fontsize=10, weight='bold',\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    if 'paradox_intensity' in data.columns:\n",
    "        paradox_data = data[data['quadrant'] == 'Transport Paradox']['paradox_intensity'].dropna()\n",
    "        n, bins, patches = ax3.hist(paradox_data, bins=20, alpha=0.7, color='#d62728', \n",
    "                                   edgecolor='black', linewidth=0.8)\n",
    "        \n",
    "        mean_val = paradox_data.mean()\n",
    "        std_val = paradox_data.std()\n",
    "        ax3.axvline(mean_val, color='black', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean = {mean_val:.3f}')\n",
    "        ax3.axvline(mean_val + std_val, color='gray', linestyle=':', linewidth=1.5,\n",
    "                   label=f'±1 SD = {std_val:.3f}')\n",
    "        ax3.axvline(mean_val - std_val, color='gray', linestyle=':', linewidth=1.5)\n",
    "        \n",
    "        ax3.set_title('Transport Paradox Intensity Distribution', fontweight='bold', pad=15)\n",
    "        ax3.set_xlabel('Paradox Intensity Score (0-1)')\n",
    "        ax3.set_ylabel('Number of LSOAs')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        shapiro_stat, shapiro_p = stats.shapiro(paradox_data.sample(min(5000, len(paradox_data))))\n",
    "        ax3.text(0.98, 0.98, f'n = {len(paradox_data)}\\nShapiro-Wilk: W = {shapiro_stat:.3f}', \n",
    "                 transform=ax3.transAxes, ha='right', va='top', fontsize=10,\n",
    "                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    priority_counts = data['priority_level'].value_counts().reindex(priority_order)\n",
    "    colors_pie = ['#8B0000', '#DC143C', '#FF8C00', '#228B22', '#4682B4']\n",
    "    \n",
    "    wedges, texts, autotexts = ax4.pie(priority_counts.values, \n",
    "                                       labels=priority_counts.index,\n",
    "                                       autopct='%1.1f%%',\n",
    "                                       colors=colors_pie,\n",
    "                                       startangle=90,\n",
    "                                       textprops={'fontsize': 10})\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax4.set_title('Overall Priority Level Distribution', fontweight='bold', pad=15)\n",
    "    \n",
    "    total_lsoas = len(data)\n",
    "    high_priority = priority_counts[['Very High', 'High']].sum()\n",
    "    ax4.text(0.02, 0.02, f'Total LSOAs: {total_lsoas:,}\\nHigh Priority: {high_priority:,} ({high_priority/total_lsoas*100:.1f}%)',\n",
    "             transform=ax4.transAxes, fontsize=10,\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_academic_figures(data, gdf_data):\n",
    "    print(\"Generating academic figures...\")\n",
    "    \n",
    "    print(\"Creating Figure 1: PTAL vs IMD Scatter Plot...\")\n",
    "    fig1 = create_figure1_ptal_imd_scatter(data)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Creating Figure 2: Policy Priority Index Map...\")\n",
    "    fig2 = create_figure2_priority_map(gdf_data)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Creating Figure 3: Statistical Analysis...\")\n",
    "    fig3 = create_figure3_statistical_heatmap(data)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig1, fig2, fig3\n",
    "\n",
    "fig1, fig2, fig3 = generate_academic_figures(merged_gdf_improved, merged_gdf_improved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Policy Priority Index Sensitivity Analysis \n",
    "# =============================================================================\n",
    "\n",
    "def sensitivity_analysis_priority_index_fixed(data, gdf, n_iter=100, noise_level=0.1):\n",
    "    baseline_quantiles = data['policy_priority_score'].quantile([0.2, 0.4, 0.6, 0.8]).values\n",
    "    \n",
    "    data['priority_baseline'] = pd.cut(\n",
    "        data['policy_priority_score'],\n",
    "        bins=[-np.inf, baseline_quantiles[0], baseline_quantiles[1], \n",
    "              baseline_quantiles[2], baseline_quantiles[3], np.inf],\n",
    "        labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "    )\n",
    "    \n",
    "    changes_record = pd.DataFrame(0, index=data.index, columns=['change_count'])\n",
    "    results = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        perturbed_score = data['policy_priority_score'] * (\n",
    "            1 + np.random.uniform(-noise_level, noise_level, size=len(data))\n",
    "        )\n",
    "        \n",
    "        perturbed_priority = pd.cut(\n",
    "            perturbed_score,\n",
    "            bins=[-np.inf, baseline_quantiles[0], baseline_quantiles[1], \n",
    "                  baseline_quantiles[2], baseline_quantiles[3], np.inf],\n",
    "            labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "        )\n",
    "        \n",
    "        perturbed_counts = perturbed_priority.value_counts(normalize=True).sort_index()\n",
    "        results.append(perturbed_counts)\n",
    "        \n",
    "        changed = (perturbed_priority != data['priority_baseline'])\n",
    "        changes_record['change_count'] += changed.astype(int)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    results_df.plot(kind='box', ax=ax[0], grid=True, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='#87CEEB', alpha=0.6))\n",
    "    ax[0].set_title(\"Distribution of Priority Levels under Weight Perturbation\", fontweight='bold')\n",
    "    ax[0].set_ylabel(\"Proportion of LSOAs\")\n",
    "    \n",
    "    base_counts = data['priority_baseline'].value_counts(normalize=True).sort_index()\n",
    "    for i, level in enumerate(results_df.columns):\n",
    "        ax[0].plot(i+1, base_counts[level], 'ro', label=\"Baseline\" if i == 0 else \"\")\n",
    "    ax[0].legend()\n",
    "    \n",
    "    sns.heatmap(results_df.T, cmap='YlOrRd', ax=ax[1], cbar_kws={'label': 'Proportion'})\n",
    "    ax[1].set_title(\"Heatmap of Priority Distribution across Iterations\", fontweight='bold')\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].set_ylabel(\"Priority Level\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(results_df.describe().round(3))\n",
    "    \n",
    "    \n",
    "    gdf['change_frequency'] = changes_record['change_count'] / n_iter  # 变化频率 (0-1)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    gdf.plot(column='change_frequency', cmap='Reds', legend=True, ax=ax,\n",
    "             legend_kwds={'label': \"Probability of Priority Change\", 'shrink': 0.6},\n",
    "             edgecolor='white', linewidth=0.1)\n",
    "    \n",
    "    ax.set_title(\"Spatial Sensitivity of Policy Priority Index\\n(Change Frequency across Perturbations)\",\n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df, gdf\n",
    "\n",
    "\n",
    "sensitivity_results, merged_gdf_improved = sensitivity_analysis_priority_index_fixed(\n",
    "    merged_gdf_improved, merged_gdf_improved, n_iter=200, noise_level=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5fb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figures: Global Moran’s I and LISA Cluster Maps (IMD Quintile & PTAL)\n",
    "# =============================================================================\n",
    "\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'Times', 'Georgia', 'STSong', 'SimSun']\n",
    "mpl.rcParams['font.size'] = 10\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid', rc={'font.family': 'serif'})\n",
    "\n",
    "colors_quad_academic = {\n",
    "   \"High-High\": \"#b2182b\",   \n",
    "     \"Low-Low\":   \"#2166ac\",   \n",
    "     \"High-Low\":  \"#ef8a62\",   \n",
    "     \"Low-High\":  \"#67a9cf\",   \n",
    "     \"Not Significant\": \"#d9d9d9\"\n",
    "}\n",
    "\n",
    "\n",
    "def build_weights(gdf, k_fallback=6):\n",
    "    w = libpysal.weights.Queen.from_dataframe(gdf, ids=gdf.index.tolist())\n",
    "    islands = w.islands\n",
    "    if islands:\n",
    "        coords = np.column_stack([gdf.geometry.centroid.x.values, gdf.geometry.centroid.y.values])\n",
    "        w = libpysal.weights.KNN.from_array(coords, k=k_fallback, ids=gdf.index.tolist())\n",
    "    w.transform = \"r\"\n",
    "    return w\n",
    "\n",
    "def moran_and_lisa_panels(gdf, value_cols, labels, permutations=999, alpha=0.05):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 14))\n",
    "\n",
    "    results = {}\n",
    "    for i, (value_col, var_label) in enumerate(zip(value_cols, labels)):\n",
    "        data = gdf[[value_col, \"geometry\"]].dropna().copy().reset_index(drop=True)\n",
    "\n",
    "        w = build_weights(data)\n",
    "        y = data[value_col].values.astype(float)\n",
    "        moran = Moran(y, w, permutations=permutations)\n",
    "        lisa = Moran_Local(y, w, permutations=permutations)\n",
    "\n",
    "        data[\"lisa_I\"] = lisa.Is\n",
    "        data[\"lisa_p\"] = lisa.p_sim\n",
    "        quad_map = {1: \"High-High\", 2: \"Low-High\", 3: \"Low-Low\", 4: \"High-Low\"}\n",
    "        data[\"lisa_q\"] = pd.Categorical([quad_map[q] for q in lisa.q], \n",
    "                                        categories=[\"High-High\",\"Low-Low\",\"High-Low\",\"Low-High\"], \n",
    "                                        ordered=False)\n",
    "        data[\"lisa_sig\"] = np.where(data[\"lisa_p\"] < alpha, data[\"lisa_q\"], \"Not Significant\")\n",
    "\n",
    "        print(f\"=== Global Moran's I ({var_label}) ===\")\n",
    "        print(f\"I = {moran.I:.4f}, p-value = {moran.p_sim:.4f}, permutations = {permutations}\")\n",
    "        try:\n",
    "            print(f\"z-score ≈ {moran.z_norm:.3f}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        ax = axes[i]\n",
    "        data.plot(ax=ax,\n",
    "                  color=data[\"lisa_sig\"].map(colors_quad_academic),\n",
    "                  edgecolor=\"white\", linewidth=0.1)\n",
    "        ax.set_title(f\"LISA Cluster Map ({var_label})\", fontsize=15, pad=20)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        \n",
    "        patches = [mpatches.Patch(color=colors_quad_academic[k], label=k) \n",
    "                   for k in [\"High-High\",\"Low-Low\",\"High-Low\",\"Low-High\",\"Not Significant\"]]\n",
    "        ax.legend(handles=patches, loc=\"lower left\", frameon=True)\n",
    "\n",
    "        results[var_label] = {\"gdf\": data, \"moran\": moran, \"lisa\": lisa}\n",
    "\n",
    "    plt.tight_layout(h_pad=3)\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = moran_and_lisa_panels(\n",
    "    merged_gdf_clean,\n",
    "    [\"IMD_quintile_london\", \"MEAN_PTAL_CLEAN\"],\n",
    "    [\"IMD (Quintile)\", \"PTAL\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd_values = merged_gdf_clean[\"IMD_quintile_london\"].values\n",
    "\n",
    "w = libpysal.weights.Queen.from_dataframe(merged_gdf_clean)\n",
    "w.transform = \"r\"\n",
    "\n",
    "lisa_imd = Moran_Local(imd_values, w)\n",
    "\n",
    "merged_gdf_clean[\"IMD_LISA_I\"] = lisa_imd.Is\n",
    "merged_gdf_clean[\"IMD_LISA_p\"] = lisa_imd.p_sim\n",
    "merged_gdf_clean[\"IMD_LISA_cluster\"] = lisa_imd.q\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "merged_gdf_clean.assign(\n",
    "    cluster=merged_gdf_clean[\"IMD_LISA_cluster\"].astype(str)\n",
    ").plot(\n",
    "    column=\"cluster\", categorical=True, legend=True, cmap=\"Set1\", linewidth=0.1, ax=ax\n",
    ")\n",
    "ax.set_title(\"Local Moran’s I Cluster Map (IMD)\", fontsize=14)\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3908b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd_values = merged_gdf_clean['IMD_quintile_london'].values\n",
    "\n",
    "w = libpysal.weights.Queen.from_dataframe(merged_gdf_clean)\n",
    "w.transform = 'r'\n",
    "\n",
    "lisa = Moran_Local(imd_values, w)\n",
    "\n",
    "merged_gdf_clean['lisa_I'] = lisa.Is\n",
    "merged_gdf_clean['lisa_p'] = lisa.p_sim\n",
    "merged_gdf_clean['lisa_q'] = lisa.q\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "colors = {1: 'red', 2: 'lightblue', 3: 'blue', 4: 'pink'}\n",
    "for q, color in colors.items():\n",
    "    subset = merged_gdf_clean[(merged_gdf_clean['lisa_q'] == q) & (merged_gdf_clean['lisa_p'] < 0.05)]\n",
    "    subset.plot(ax=ax, color=color, edgecolor='white', label=f'LISA Cluster {q} (p<0.05)')\n",
    "ax.set_title(\"Local Moran's I (LISA) - IMD Hot/Cold Spots\")\n",
    "ax.axis('off')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LISA–Housing Price Analysis: Join, Cluster Stats, and Paradox Identification\n",
    "# =============================================================================\n",
    "\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'Times', 'Georgia', 'STSong', 'SimSun']\n",
    "mpl.rcParams['font.size'] = 10\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid', rc={'font.family': 'serif'})\n",
    "\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "save_dir = r\"C:\\Users\\16643\\Desktop\\figures\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"Starting LISA housing price analysis...\")\n",
    "\n",
    "drop_cols = [c for c in merged_gdf_clean.columns if str(c).startswith('median_house_price')]\n",
    "if drop_cols:\n",
    "    merged_gdf_clean = merged_gdf_clean.drop(columns=drop_cols)\n",
    "\n",
    "house_price_df = pd.read_csv(\n",
    "    r\"C:\\Users\\16643\\Desktop\\data\\Median price paid for residential properties by LSOA.csv\"\n",
    ")\n",
    "\n",
    "house_price_clean = (\n",
    "    house_price_df[['LSOA code', 'Year ending Dec 2021']]\n",
    "    .rename(columns={'LSOA code': 'LSOA11CD',\n",
    "                     'Year ending Dec 2021': 'median_house_price'})\n",
    ")\n",
    "\n",
    "house_price_clean['median_house_price'] = (\n",
    "    house_price_clean['median_house_price']\n",
    "    .astype(str)\n",
    "    .str.replace(',', '')\n",
    "    .str.replace('£', '')\n",
    "    .str.strip()\n",
    "    .replace('', np.nan)\n",
    ")\n",
    "\n",
    "house_price_clean['median_house_price'] = pd.to_numeric(\n",
    "    house_price_clean['median_house_price'], errors='coerce'\n",
    ")\n",
    "house_price_clean = house_price_clean.dropna()\n",
    "\n",
    "house_price_2021 = (\n",
    "    house_price_clean\n",
    "    .merge(crosswalk_df, on='LSOA11CD', how='left')\n",
    "    .dropna(subset=['LSOA21CD'])\n",
    "    .groupby('LSOA21CD', as_index=False)['median_house_price']\n",
    "    .median()\n",
    ")\n",
    "\n",
    "merged_gdf_clean = merged_gdf_clean.merge(\n",
    "    house_price_2021[['LSOA21CD', 'median_house_price']],\n",
    "    on='LSOA21CD',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "significant_lisa = merged_gdf_clean.query(\"IMD_LISA_p < 0.05\").copy()\n",
    "\n",
    "cluster_labels = {\n",
    "    1: 'High-High (Deprivation Hotspots)',\n",
    "    2: 'Low-High (Outliers)', \n",
    "    3: 'Low-Low (Affluent Clusters)',\n",
    "    4: 'High-Low (Outliers)'\n",
    "}\n",
    "\n",
    "significant_lisa['cluster_label'] = significant_lisa['IMD_LISA_cluster'].map(cluster_labels)\n",
    "\n",
    "if significant_lisa['median_house_price'].notna().any():\n",
    "    housing_stats = (\n",
    "        significant_lisa\n",
    "        .dropna(subset=['median_house_price'])\n",
    "        .groupby('cluster_label')['median_house_price']\n",
    "        .agg(['count', 'mean', 'median', 'std'])\n",
    "        .round(0)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nHousing price statistics by LISA clusters:\")\n",
    "    print(housing_stats.to_string(formatters={\n",
    "        'mean': '£{:,.0f}'.format,\n",
    "        'median': '£{:,.0f}'.format, \n",
    "        'std': '£{:,.0f}'.format\n",
    "    }))\n",
    "\n",
    "high_high_areas = significant_lisa[\n",
    "    significant_lisa['cluster_label'] == 'High-High (Deprivation Hotspots)'\n",
    "].copy()\n",
    "\n",
    "if not high_high_areas.empty:\n",
    "    ptal_column = None\n",
    "    possible_ptal_names = ['MEAN_PTAL_CLEAN', 'MEAN_PTAL_', 'PTAL', 'mean_ptal']\n",
    "    \n",
    "    for col_name in possible_ptal_names:\n",
    "        if col_name in high_high_areas.columns:\n",
    "            ptal_column = col_name\n",
    "            break\n",
    "    \n",
    "    if ptal_column is None:\n",
    "        ptal_cols = [col for col in high_high_areas.columns if 'PTAL' in col.upper()]\n",
    "        if ptal_cols:\n",
    "            ptal_column = ptal_cols[0]\n",
    "    \n",
    "    if ptal_column and high_high_areas[[ptal_column, 'median_house_price']].dropna().shape[0] > 0:\n",
    "        ptal_median = high_high_areas[ptal_column].median()\n",
    "        price_q75 = high_high_areas['median_house_price'].quantile(0.75)\n",
    "        \n",
    "        high_high_areas['is_paradox'] = (\n",
    "            (high_high_areas[ptal_column] >= ptal_median) &\n",
    "            (high_high_areas['median_house_price'] >= price_q75)\n",
    "        )\n",
    "        \n",
    "        paradox_areas = high_high_areas[high_high_areas['is_paradox']]\n",
    "        \n",
    "        print(f\"\\nTransport Paradox analysis results:\")\n",
    "        print(f\"   Total High-High areas: {len(high_high_areas)}\")\n",
    "        print(f\"   Paradox candidates: {len(paradox_areas)}\")\n",
    "        print(f\"   Paradox proportion: {len(paradox_areas)/len(high_high_areas)*100:.1f}%\")\n",
    "        \n",
    "        if not paradox_areas.empty:\n",
    "            print(f\"   Average house price (Paradox areas): £{paradox_areas['median_house_price'].mean():,.0f}\")\n",
    "            print(f\"   Average house price (All High-High): £{high_high_areas['median_house_price'].mean():,.0f}\")\n",
    "            print(f\"   Price ratio: {paradox_areas['median_house_price'].mean() / high_high_areas['median_house_price'].mean():.2f}x\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nAnalysis Summary:\")\n",
    "print(f\"   • Total LSOA areas: {len(merged_gdf_clean):,}\")\n",
    "print(f\"   • With house price data: {merged_gdf_clean['median_house_price'].notna().sum():,}\")\n",
    "print(f\"   • Significant LISA areas: {len(significant_lisa):,}\")\n",
    "print(f\"   • High-High areas: {len(high_high_areas):,}\")\n",
    "if 'paradox_areas' in locals():\n",
    "    print(f\"   • Transport Paradox candidates: {len(paradox_areas):,}\")\n",
    "print(f\"\\nDefinition of Transport Paradox: High-High deprivation areas with both high transport accessibility and high house prices\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e16444",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# LISA boxplot\n",
    "if not significant_lisa.empty and significant_lisa['median_house_price'].notna().any():\n",
    "    plot_data = significant_lisa.dropna(subset=['median_house_price', 'cluster_label'])\n",
    "    sns.boxplot(\n",
    "        data=plot_data,\n",
    "        x='cluster_label',\n",
    "        y='median_house_price',\n",
    "        order=[label for label in cluster_labels.values() if label in plot_data['cluster_label'].values],\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title('Median House Prices across LISA Clusters', fontsize=15, pad=20)\n",
    "    axes[0].set_xlabel('')\n",
    "    axes[0].set_ylabel('Median House Price (£)')\n",
    "    axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'£{x/1000:.0f}K'))\n",
    "\n",
    "    new_labels = {\n",
    "        'High-High (Deprivation Hotspots)': 'High-High\\n(Deprivation Hotspots)',\n",
    "        'Low-High (Outliers)': 'Low-High\\n(Outliers)',\n",
    "        'Low-Low (Affluent Clusters)': 'Low-Low\\n(Affluent Clusters)',\n",
    "        'High-Low (Outliers)': 'High-Low\\n(Outliers)'\n",
    "    }\n",
    "    axes[0].set_xticklabels([new_labels.get(t.get_text(), t.get_text()) for t in axes[0].get_xticklabels()])\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No Data Available', ha='center', va='center',\n",
    "                 transform=axes[0].transAxes, fontsize=12)\n",
    "\n",
    "# High-High scatter\n",
    "if (not high_high_areas.empty and 'ptal_column' in locals() and \n",
    "    high_high_areas[[ptal_column, 'median_house_price']].dropna().shape[0] > 0):\n",
    "\n",
    "    plot_data = high_high_areas.dropna(subset=[ptal_column, 'median_house_price'])\n",
    "    colors = ['lightcoral' if not paradox else 'darkred' for paradox in plot_data['is_paradox']]\n",
    "\n",
    "    axes[1].scatter(\n",
    "        plot_data[ptal_column],\n",
    "        plot_data['median_house_price'],\n",
    "        c=colors, s=60, alpha=0.7, edgecolors='white', linewidth=0.5\n",
    "    )\n",
    "    axes[1].axhline(price_q75, color='gray', ls='--', lw=1, alpha=0.8,\n",
    "                    label=f'Price Q75: £{price_q75:,.0f}')\n",
    "    axes[1].axvline(ptal_median, color='gray', ls='--', lw=1, alpha=0.8,\n",
    "                    label=f'PTAL Median: {ptal_median:.2f}')\n",
    "\n",
    "    axes[1].text(0.02, 0.98, f'Transport Paradox:\\n{len(paradox_areas)} areas',\n",
    "                 transform=axes[1].transAxes, fontsize=10, verticalalignment='top',\n",
    "                 bbox=dict(boxstyle='round,pad=0.3', facecolor='darkred', alpha=0.1))\n",
    "\n",
    "    axes[1].set_title('PTAL and House Prices in High-High IMD Areas',\n",
    "                      fontsize=15, pad=20)\n",
    "    axes[1].set_xlabel('PTAL Score (Transport Accessibility)')\n",
    "    axes[1].set_ylabel('Median House Price (£)')\n",
    "    axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'£{x/1000:.0f}K'))\n",
    "\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral',\n",
    "                   markersize=8, label='Normal'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='darkred',\n",
    "                   markersize=8, label='Paradox Candidate')\n",
    "    ]\n",
    "    axes[1].legend(\n",
    "        handles=legend_elements,\n",
    "        title='Area Type',\n",
    "        loc='center left',\n",
    "        bbox_to_anchor=(1, 0.5),\n",
    "        frameon=True\n",
    "    )\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No High-High Areas\\nwith Complete Data',\n",
    "                 ha='center', va='center', transform=axes[1].transAxes, fontsize=12)\n",
    "\n",
    "plt.tight_layout(h_pad=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21193ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- setup / paths ---\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "save_dir = r\"C:\\Users\\16643\\Desktop\\figures\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"Starting LISA housing price analysis based on IMD quintile...\")\n",
    "\n",
    "# --- sanity checks for required columns ---\n",
    "required_columns = ['IMD_quintile_london']\n",
    "missing_cols = [col for col in required_columns if col not in merged_gdf_clean.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Missing required columns: {missing_cols}\")\n",
    "    print(f\"Available columns: {[col for col in merged_gdf_clean.columns if 'IMD' in col or 'quintile' in col]}\")\n",
    "else:\n",
    "    # --- input distribution overview ---\n",
    "    print(f\"Using IMD quintile data: {merged_gdf_clean['IMD_quintile_london'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "    # --- build spatial weights and compute Local Moran's I ---\n",
    "    print(\"Building spatial weights matrix...\")\n",
    "    w = libpysal.weights.Queen.from_dataframe(merged_gdf_clean)\n",
    "    w.transform = 'r'\n",
    "    print(f\"Spatial weights: {w.n} areas, mean neighbors: {w.mean_neighbors:.2f}\")\n",
    "\n",
    "    print(\"Computing Local Moran's I...\")\n",
    "    imd_values = merged_gdf_clean['IMD_quintile_london'].values\n",
    "    lisa = Moran_Local(imd_values, w)\n",
    "\n",
    "    merged_gdf_clean['lisa_I'] = lisa.Is\n",
    "    merged_gdf_clean['lisa_p'] = lisa.p_sim\n",
    "    merged_gdf_clean['lisa_q'] = lisa.q\n",
    "\n",
    "    print(\"LISA analysis completed\")\n",
    "    print(f\"Significant areas (p<0.05): {(merged_gdf_clean['lisa_p'] < 0.05).sum()}/{len(merged_gdf_clean)}\")\n",
    "\n",
    "    cluster_counts = (\n",
    "        merged_gdf_clean[merged_gdf_clean['lisa_p'] < 0.05]['lisa_q']\n",
    "        .value_counts().sort_index()\n",
    "    )\n",
    "    print(f\"Cluster distribution: {cluster_counts.to_dict()}\")\n",
    "\n",
    "    # --- house price processing ---\n",
    "    print(\"\\nProcessing house price data...\")\n",
    "\n",
    "    drop_cols = [c for c in merged_gdf_clean.columns if str(c).startswith('median_house_price')]\n",
    "    if drop_cols:\n",
    "        merged_gdf_clean = merged_gdf_clean.drop(columns=drop_cols)\n",
    "\n",
    "    house_price_file = r\"C:\\Users\\16643\\Desktop\\data\\Median price paid for residential properties by LSOA.csv\"\n",
    "\n",
    "    if not os.path.exists(house_price_file):\n",
    "        print(f\"House price data file not found: {house_price_file}\")\n",
    "    else:\n",
    "        house_price_df = pd.read_csv(house_price_file)\n",
    "\n",
    "        house_price_clean = (\n",
    "            house_price_df[['LSOA code', 'Year ending Dec 2021']]\n",
    "            .rename(columns={'LSOA code': 'LSOA11CD',\n",
    "                             'Year ending Dec 2021': 'median_house_price'})\n",
    "        )\n",
    "\n",
    "        house_price_clean['median_house_price'] = (\n",
    "            house_price_clean['median_house_price']\n",
    "            .astype(str)\n",
    "            .str.replace(',', '')\n",
    "            .str.replace('£', '')\n",
    "            .str.strip()\n",
    "            .replace('', np.nan)\n",
    "        )\n",
    "\n",
    "        house_price_clean['median_house_price'] = pd.to_numeric(\n",
    "            house_price_clean['median_house_price'], errors='coerce'\n",
    "        )\n",
    "        house_price_clean = house_price_clean.dropna()\n",
    "\n",
    "        house_price_2021 = (\n",
    "            house_price_clean\n",
    "            .merge(crosswalk_df, on='LSOA11CD', how='left')\n",
    "            .dropna(subset=['LSOA21CD'])\n",
    "            .groupby('LSOA21CD', as_index=False)['median_house_price']\n",
    "            .median()\n",
    "        )\n",
    "\n",
    "        merged_gdf_clean = merged_gdf_clean.merge(\n",
    "            house_price_2021[['LSOA21CD', 'median_house_price']],\n",
    "            on='LSOA21CD',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        price_coverage = merged_gdf_clean['median_house_price'].notna().sum()\n",
    "        print(f\"House price data merged - coverage: {price_coverage}/{len(merged_gdf_clean)} \"\n",
    "              f\"({price_coverage/len(merged_gdf_clean)*100:.1f}%)\")\n",
    "\n",
    "        # --- LISA cluster housing price analysis ---\n",
    "        print(\"\\nLISA cluster housing price analysis...\")\n",
    "\n",
    "        significant_lisa = merged_gdf_clean[merged_gdf_clean['lisa_p'] < 0.05].copy()\n",
    "\n",
    "        cluster_labels = {\n",
    "            1: 'High-High (IMD Hotspots)',\n",
    "            2: 'Low-High (Outliers)',\n",
    "            3: 'Low-Low (Affluent Clusters)',\n",
    "            4: 'High-Low (Outliers)'\n",
    "        }\n",
    "\n",
    "        significant_lisa['cluster_label'] = significant_lisa['lisa_q'].map(cluster_labels)\n",
    "\n",
    "        if significant_lisa['median_house_price'].notna().any():\n",
    "            housing_stats = (\n",
    "                significant_lisa\n",
    "                .dropna(subset=['median_house_price'])\n",
    "                .groupby('cluster_label')['median_house_price']\n",
    "                .agg(['count', 'mean', 'median', 'std'])\n",
    "                .round(0)\n",
    "            )\n",
    "\n",
    "            print(\"Housing statistics by LISA clusters:\")\n",
    "            for cluster, stats in housing_stats.iterrows():\n",
    "                print(f\"   {cluster}:\")\n",
    "                print(f\"     Count: {int(stats['count'])}\")\n",
    "                print(f\"     Mean price: £{stats['mean']:,.0f}\")\n",
    "                print(f\"     Median price: £{stats['median']:,.0f}\")\n",
    "                if not pd.isna(stats['std']):\n",
    "                    print(f\"     Std. dev.: £{stats['std']:,.0f}\")\n",
    "\n",
    "        # --- Transport Paradox analysis ---\n",
    "        print(\"\\nTransport Paradox analysis...\")\n",
    "\n",
    "        high_high_areas = significant_lisa[\n",
    "            significant_lisa['cluster_label'] == 'High-High (IMD Hotspots)'\n",
    "        ].copy()\n",
    "\n",
    "        print(f\"IMD deprivation hotspots: {len(high_high_areas)}\")\n",
    "\n",
    "        if not high_high_areas.empty:\n",
    "            ptal_column = None\n",
    "            possible_ptal_names = ['MEAN_PTAL_CLEAN', 'MEAN_PTAL_', 'PTAL', 'mean_ptal']\n",
    "\n",
    "            for col_name in possible_ptal_names:\n",
    "                if col_name in high_high_areas.columns:\n",
    "                    ptal_column = col_name\n",
    "                    break\n",
    "\n",
    "            if ptal_column is None:\n",
    "                ptal_cols = [col for col in high_high_areas.columns if 'PTAL' in col.upper()]\n",
    "                if ptal_cols:\n",
    "                    ptal_column = ptal_cols[0]\n",
    "\n",
    "            print(f\"Using PTAL column: {ptal_column}\")\n",
    "\n",
    "            if ptal_column and high_high_areas[[ptal_column, 'median_house_price']].dropna().shape[0] > 0:\n",
    "                complete_data = high_high_areas.dropna(subset=[ptal_column, 'median_house_price'])\n",
    "                print(f\"Complete cases: {len(complete_data)}/{len(high_high_areas)}\")\n",
    "\n",
    "                ptal_median = complete_data[ptal_column].median()\n",
    "                price_q75 = complete_data['median_house_price'].quantile(0.75)\n",
    "\n",
    "                print(f\"PTAL median: {ptal_median:.2f}\")\n",
    "                print(f\"House price 75th percentile: £{price_q75:,.0f}\")\n",
    "\n",
    "                high_high_areas['is_paradox'] = (\n",
    "                    (high_high_areas[ptal_column] >= ptal_median) &\n",
    "                    (high_high_areas['median_house_price'] >= price_q75)\n",
    "                )\n",
    "\n",
    "                paradox_areas = high_high_areas[high_high_areas['is_paradox'] == True]\n",
    "\n",
    "                print(\"\\nTransport Paradox results:\")\n",
    "                print(f\"Number of candidate areas: {len(paradox_areas)}\")\n",
    "                print(f\"Share of hotspots: {len(paradox_areas)/len(high_high_areas)*100:.1f}%\")\n",
    "\n",
    "                if not paradox_areas.empty:\n",
    "                    print(f\"Average price (Paradox areas): £{paradox_areas['median_house_price'].mean():,.0f}\")\n",
    "                    print(f\"Average price (All hotspots): £{complete_data['median_house_price'].mean():,.0f}\")\n",
    "                    print(f\"Price ratio: {paradox_areas['median_house_price'].mean() / complete_data['median_house_price'].mean():.2f}x\")\n",
    "                    print(f\"Average PTAL score: {paradox_areas[ptal_column].mean():.2f}\")\n",
    "\n",
    "# --- final textual summary ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMD-based LISA analysis summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total LSOA areas: {len(merged_gdf_clean):,}\")\n",
    "\n",
    "if 'significant_lisa' in locals():\n",
    "    print(f\"Significant LISA areas: {len(significant_lisa):,} \"\n",
    "          f\"({len(significant_lisa)/len(merged_gdf_clean)*100:.1f}%)\")\n",
    "\n",
    "if 'price_coverage' in locals():\n",
    "    print(f\"Areas with house price data: {price_coverage:,} \"\n",
    "          f\"({price_coverage/len(merged_gdf_clean)*100:.1f}%)\")\n",
    "\n",
    "if 'significant_lisa' in locals() and not significant_lisa.empty:\n",
    "    cluster_labels = {\n",
    "        1: 'High-High (IMD Hotspots)',\n",
    "        2: 'Low-High (Outliers)',\n",
    "        3: 'Low-Low (Affluent Clusters)',\n",
    "        4: 'High-Low (Outliers)'\n",
    "    }\n",
    "    for cluster_id, label in cluster_labels.items():\n",
    "        count = len(significant_lisa[significant_lisa['lisa_q'] == cluster_id])\n",
    "        if count > 0:\n",
    "            print(f\"{label}: {count}\")\n",
    "\n",
    "if 'high_high_areas' in locals():\n",
    "    print(f\"\\nIMD deprivation hotspots: {len(high_high_areas)}\")\n",
    "    if 'paradox_areas' in locals():\n",
    "        print(f\"Transport Paradox candidates: {len(paradox_areas)}\")\n",
    "\n",
    "print(\"\\nTransport Paradox definition:\")\n",
    "print(\"   Within IMD deprivation hotspots, areas that simultaneously have:\")\n",
    "print(\"   • High transport accessibility (PTAL ≥ median)\")\n",
    "print(\"   • High house prices (≥ 75th percentile)\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf_clean['Borough'] = merged_gdf_clean['LSOA21NM'].str[:6]  \n",
    "\n",
    "formula = \"IMD_quintile_london ~ MEAN_PTAL_CLEAN + C(Borough)\"\n",
    "ols_model = smf.ols(formula=formula, data=merged_gdf_clean).fit(cov_type='HC3')  \n",
    "print(ols_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d545be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Spatial Regression: OLS, Spatial Lag (SAR), and Spatial Error (SEM) — IMD ~ PTAL (Queen Weights)\n",
    "# =============================================================================\n",
    "\n",
    "y = merged_gdf_clean[\"IMD_quintile_london\"].values.reshape(-1, 1)\n",
    "X = merged_gdf_clean[[\"MEAN_PTAL_CLEAN\"]].values\n",
    "\n",
    "w = libpysal.weights.Queen.from_dataframe(merged_gdf_clean)\n",
    "w.transform = \"r\"\n",
    "\n",
    "ols_model = OLS(y, X, w=w, name_y=\"IMD\", name_x=[\"PTAL\"])\n",
    "print(\"=== OLS Results ===\")\n",
    "print(ols_model.summary)\n",
    "\n",
    "lag_model = ML_Lag(y, X, w=w, name_y=\"IMD\", name_x=[\"PTAL\"])\n",
    "print(\"\\n=== Spatial Lag Model Results ===\")\n",
    "print(lag_model.summary)\n",
    "\n",
    "error_model = ML_Error(y, X, w=w, name_y=\"IMD\", name_x=[\"PTAL\"])\n",
    "print(\"\\n=== Spatial Error Model Results ===\")\n",
    "print(error_model.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646809c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theil_index(x):\n",
    "    x = np.array(x)\n",
    "    x = x[x > 0]\n",
    "    mean_x = np.mean(x)\n",
    "    return np.sum((x / mean_x) * np.log(x / mean_x)) / len(x)\n",
    "\n",
    "def atkinson_index(x, epsilon=0.5):\n",
    "    x = np.array(x)\n",
    "    x = x[x > 0]\n",
    "    mean_x = np.mean(x)\n",
    "    return 1 - (np.mean((x / mean_x) ** (1 - epsilon))) ** (1 / (1 - epsilon))\n",
    "\n",
    "ineq_results = {}\n",
    "for q in merged_gdf_clean['IMD_quintile_london'].unique():\n",
    "    subset = merged_gdf_clean[merged_gdf_clean['IMD_quintile_london'] == q]\n",
    "    ptal_vals = subset['MEAN_PTAL_CLEAN'].values\n",
    "    ineq_results[q] = {\n",
    "        \"Theil\": theil_index(ptal_vals),\n",
    "        \"Atkinson\": atkinson_index(ptal_vals)\n",
    "    }\n",
    "\n",
    "for q, v in ineq_results.items():\n",
    "    print(f\"IMD Quintile {q}: Theil = {v['Theil']:.3f}, Atkinson = {v['Atkinson']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa912627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Analysis: Theil and Atkinson Indices of PTAL Distribution — \n",
    "# Bar/Line Plots across IMD Quintiles with Summary Table\n",
    "# =============================================================================\n",
    "\n",
    "data = {\n",
    "    \"IMD Quintile\": [1, 2, 3, 4, 5],\n",
    "    \"Theil\": [0.113, 0.113, 0.122, 0.136, 0.146],\n",
    "    \"Atkinson\": [0.057, 0.057, 0.061, 0.068, 0.071]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bar_width = 0.35\n",
    "x = range(len(df))\n",
    "\n",
    "ax.bar([i - bar_width/2 for i in x], df[\"Theil\"], width=bar_width, label=\"Theil Index\")\n",
    "ax.bar([i + bar_width/2 for i in x], df[\"Atkinson\"], width=bar_width, label=\"Atkinson Index\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df[\"IMD Quintile\"])\n",
    "ax.set_xlabel(\"IMD Quintile (1=Least deprived, 5=Most deprived)\")\n",
    "ax.set_ylabel(\"Inequality Index Value\")\n",
    "ax.set_title(\"Theil and Atkinson Indices of PTAL Distribution\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(df[\"IMD Quintile\"], df[\"Theil\"], marker=\"o\", label=\"Theil Index\")\n",
    "ax.plot(df[\"IMD Quintile\"], df[\"Atkinson\"], marker=\"s\", label=\"Atkinson Index\")\n",
    "\n",
    "ax.set_xlabel(\"IMD Quintile (1=Least deprived, 5=Most deprived)\")\n",
    "ax.set_ylabel(\"Inequality Index Value\")\n",
    "ax.set_title(\"Trends of Transport Inequality across IMD Quintiles\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Inequality indices by IMD Quintile ===\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92531f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Figure : Correlation between PTAL and IMD domains\n",
    "# =========================\n",
    "\n",
    "# Serif font settings\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'Times', 'Georgia', 'STSong', 'SimSun']\n",
    "mpl.rcParams['font.size'] = 10\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Correlation data\n",
    "correlation_data = {\n",
    "    \"Environment\": -0.554,\n",
    "    \"Crime\": -0.314,\n",
    "    \"Health\": -0.250,\n",
    "    \"Income\": -0.175,\n",
    "    \"Employment\": -0.121,\n",
    "    \"Housing\": -0.052,\n",
    "    \"Education\": 0.087\n",
    "}\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "sorted_domains = sorted(correlation_data.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "domains = [item[0] for item in sorted_domains]\n",
    "correlations = [item[1] for item in sorted_domains]\n",
    "\n",
    "# Colors: blue for negative, red for positive\n",
    "colors = ['red' if corr > 0 else 'steelblue' for corr in correlations]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(domains, correlations, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Labels on bars\n",
    "for i, (domain, corr) in enumerate(zip(domains, correlations)):\n",
    "    ax.text(corr + (0.01 if corr > 0 else -0.01), i, f'{corr:.3f}', \n",
    "            ha='left' if corr > 0 else 'right', va='center', fontweight='normal')\n",
    "\n",
    "# Reference lines\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax.axvline(x=-0.3, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=-0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Titles and labels\n",
    "ax.set_title('Correlation between PTAL and IMD Domains', fontsize=15, pad=20)\n",
    "ax.set_xlabel('Pearson Correlation Coefficient (r)', fontsize=12)\n",
    "ax.set_ylabel('Deprivation Domain', fontsize=12)\n",
    "\n",
    "# X-axis limits and ticks\n",
    "ax.set_xlim(-0.6, 0.15)\n",
    "ax.set_xticks(np.arange(-0.6, 0.2, 0.1))\n",
    "\n",
    "# Grid\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Legend on the right\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', alpha=0.7, label='Negative correlation'),\n",
    "    Patch(facecolor='red', alpha=0.7, label='Positive correlation')\n",
    "]\n",
    "ax.legend(handles=legend_elements, title='Correlation', \n",
    "          loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=True)\n",
    "\n",
    "# Note\n",
    "plt.figtext(0.02, 0.02, 'Note: All correlations significant at p < 0.001', \n",
    "            fontsize=9, style='italic', ha='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
